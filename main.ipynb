{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "71e5dc65",
      "metadata": {},
      "source": [
        "# Video Semantic Search with SigLIP 2\n",
        "\n",
        "Search inside videos using natural language queries powered by **SigLIP 2** + **ChromaDB**\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "INDEXING PIPELINE (Offline)\n",
        "┌─────────────────────────────────────────────────────────────────────────────┐\n",
        "│  Video File ──► OpenCV ──► Frames + Timestamps ──► SigLIP2 ──► ChromaDB    │\n",
        "│                 extract      (sampled @ 1fps)       embed      store        │\n",
        "└─────────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "QUERY PIPELINE (Online)\n",
        "┌─────────────────────────────────────────────────────────────────────────────┐\n",
        "│  Text Query ──► SigLIP2 ──► Query Vector ──► ChromaDB ──► Top-K Results    │\n",
        "│                 embed                         search      + timestamps      │\n",
        "└─────────────────────────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "## Tech Stack\n",
        "\n",
        "| Component | Technology | Purpose |\n",
        "|-----------|------------|---------|\n",
        "| Frame Extraction | OpenCV | Read video, extract frames at configurable FPS |\n",
        "| Embedding Model | SigLIP 2 (google/siglip2-base-patch16-256) | Image & text embeddings in shared 768-dim space |\n",
        "| Vector Database | ChromaDB | Store & search embeddings with HNSW index |\n",
        "| Deep Learning | PyTorch + Transformers | Model inference with GPU acceleration |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3de8743",
      "metadata": {},
      "source": [
        "## 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1eaf4a67",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using CPython 3.12.4 interpreter at: \u001b[36mC:\\Users\\kirub\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\u001b[39m\n",
            "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
            "\u001b[2mResolved \u001b[1m117 packages\u001b[0m \u001b[2min 4.50s\u001b[0m\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m setuptools \u001b[2m(1.0MiB)\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m chromadb \u001b[2m(20.4MiB)\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m onnxruntime \u001b[2m(12.8MiB)\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m grpcio \u001b[2m(4.5MiB)\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m pydantic-core \u001b[2m(1.9MiB)\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m torch \u001b[2m(108.5MiB)\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m transformers \u001b[2m(9.7MiB)\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m matplotlib \u001b[2m(7.8MiB)\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m tokenizers \u001b[2m(2.6MiB)\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m fonttools \u001b[2m(2.2MiB)\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m networkx \u001b[2m(2.0MiB)\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m hf-xet \u001b[2m(2.8MiB)\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m opencv-python \u001b[2m(38.3MiB)\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m pillow \u001b[2m(6.7MiB)\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m kubernetes \u001b[2m(1.9MiB)\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m numpy \u001b[2m(11.7MiB)\u001b[0m\n",
            " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m pydantic-core\n",
            " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m fonttools\n",
            " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m tokenizers\n",
            " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m hf-xet\n",
            " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m setuptools\n",
            " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m networkx\n",
            " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m kubernetes\n",
            " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m grpcio\n",
            " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m pillow\n",
            " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m matplotlib\n",
            " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m transformers\n",
            " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m numpy\n",
            " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m onnxruntime\n",
            " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m chromadb\n",
            " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m opencv-python\n",
            " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m torch\n",
            "\u001b[2mPrepared \u001b[1m60 packages\u001b[0m \u001b[2min 1m 52s\u001b[0m\u001b[0m\n",
            "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
            "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
            "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
            "\u001b[2mInstalled \u001b[1m97 packages\u001b[0m \u001b[2min 15.05s\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.12.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==25.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbackoff\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbcrypt\u001b[0m\u001b[2m==5.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbuild\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2026.1.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mchromadb\u001b[0m\u001b[2m==1.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcolorama\u001b[0m\u001b[2m==0.4.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcoloredlogs\u001b[0m\u001b[2m==15.0.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcontourpy\u001b[0m\u001b[2m==1.3.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcycler\u001b[0m\u001b[2m==0.12.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdistro\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdurationpy\u001b[0m\u001b[2m==0.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mflatbuffers\u001b[0m\u001b[2m==25.12.19\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfonttools\u001b[0m\u001b[2m==4.61.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2026.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgoogleapis-common-protos\u001b[0m\u001b[2m==1.72.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgrpcio\u001b[0m\u001b[2m==1.76.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mh11\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhttpcore\u001b[0m\u001b[2m==1.0.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhttptools\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==1.3.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhumanfriendly\u001b[0m\u001b[2m==10.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.11\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.7.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mimportlib-resources\u001b[0m\u001b[2m==6.5.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjsonschema\u001b[0m\u001b[2m==4.26.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjsonschema-specifications\u001b[0m\u001b[2m==2025.9.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mkiwisolver\u001b[0m\u001b[2m==1.4.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mkubernetes\u001b[0m\u001b[2m==35.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==4.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.10.8\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmdurl\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmmh3\u001b[0m\u001b[2m==5.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.6.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1moauthlib\u001b[0m\u001b[2m==3.3.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1monnxruntime\u001b[0m\u001b[2m==1.23.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopencv-python\u001b[0m\u001b[2m==4.13.0.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-api\u001b[0m\u001b[2m==1.39.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-common\u001b[0m\u001b[2m==1.39.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-grpc\u001b[0m\u001b[2m==1.39.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-proto\u001b[0m\u001b[2m==1.39.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-sdk\u001b[0m\u001b[2m==1.39.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions\u001b[0m\u001b[2m==0.60b1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1morjson\u001b[0m\u001b[2m==3.11.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1moverrides\u001b[0m\u001b[2m==7.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==26.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==12.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mposthog\u001b[0m\u001b[2m==5.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.33.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpybase64\u001b[0m\u001b[2m==1.4.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.12.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.41.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyparsing\u001b[0m\u001b[2m==3.3.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpypika\u001b[0m\u001b[2m==0.50.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyproject-hooks\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyreadline3\u001b[0m\u001b[2m==3.5.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mreferencing\u001b[0m\u001b[2m==0.37.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2026.1.15\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrequests-oauthlib\u001b[0m\u001b[2m==2.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==14.3.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrpds-py\u001b[0m\u001b[2m==0.30.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==80.10.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mshellingham\u001b[0m\u001b[2m==1.5.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtenacity\u001b[0m\u001b[2m==9.1.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.10.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==5.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.21.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyper-slim\u001b[0m\u001b[2m==0.21.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.15.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.6.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1muvicorn\u001b[0m\u001b[2m==0.40.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwatchfiles\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwebsocket-client\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==16.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mzipp\u001b[0m\u001b[2m==3.23.0\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies (uncomment if needed)\n",
        "# %pip install torch transformers Pillow opencv-python chromadb numpy tqdm matplotlib\n",
        "!uv add torch transformers Pillow opencv-python chromadb numpy tqdm matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "047742f8",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from datetime import timedelta\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import chromadb\n",
        "from transformers import AutoProcessor, AutoModel\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check device - prefer CUDA for faster inference\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6acb5e0",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "All configurable parameters in one place with justifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "313393eb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: google/siglip2-base-patch16-256\n",
            "Sample rate: 1.0 fps\n",
            "Batch size: 16\n",
            "Videos directory: data/videos\n",
            "Database directory: data/chroma_db\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# MODEL CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# SigLIP 2 base model - fastest variant with 768-dim embeddings\n",
        "# Alternatives: siglip2-large-patch16-384 (1024-dim), siglip2-so400m-patch14-384 (1152-dim)\n",
        "MODEL_NAME = \"google/siglip2-base-patch16-256\"\n",
        "\n",
        "# Embedding dimension for base model (changes with model variant)\n",
        "EMBEDDING_DIM = 768\n",
        "\n",
        "# =============================================================================\n",
        "# FRAME EXTRACTION CONFIGURATION  \n",
        "# =============================================================================\n",
        "\n",
        "# Sample rate in frames per second\n",
        "# 1.0 = extract 1 frame every second (good balance of coverage vs storage)\n",
        "# 2.0 = extract 2 frames per second (more granular, 2x storage)\n",
        "# 0.5 = extract 1 frame every 2 seconds (less granular, half storage)\n",
        "SAMPLE_RATE = 1.0\n",
        "\n",
        "# =============================================================================\n",
        "# EMBEDDING CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Batch size for processing frames through the model\n",
        "# 16 works well for 8GB VRAM with base model in float16\n",
        "# Reduce to 8 if you encounter OOM errors\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# =============================================================================\n",
        "# SEARCH CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Number of results to return from similarity search\n",
        "TOP_K = 10\n",
        "\n",
        "# =============================================================================\n",
        "# STORAGE CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Directory structure\n",
        "DATA_DIR = Path(\"./data\")\n",
        "VIDEOS_DIR = DATA_DIR / \"videos\"\n",
        "DB_DIR = DATA_DIR / \"chroma_db\"\n",
        "\n",
        "# Create directories\n",
        "VIDEOS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "DB_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Sample rate: {SAMPLE_RATE} fps\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Videos directory: {VIDEOS_DIR}\")\n",
        "print(f\"Database directory: {DB_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65046375",
      "metadata": {},
      "source": [
        "## 3. Load SigLIP 2 Model\n",
        "\n",
        "Load the vision-language encoder with memory optimizations:\n",
        "- **float16**: Reduces memory usage by 50% with minimal quality loss\n",
        "- **SDPA**: Scaled Dot-Product Attention for efficient GPU computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37d069e8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: google/siglip2-base-patch16-256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afe62ba66d804bcdb6ac9b8402f2c03a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d16f9003b8814b3a9e48cd223c419a62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8515c300e49548228e80bf00a0704e56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f44b613897e4596ad826238676ac236",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a0bde9d2f7146f897dc45cf2b521b3a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e973f90f94545febfa91e68976178d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/276 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9508fc62e52441a8a1bbb1d02b42e5f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n",
            "Model device: cpu\n",
            "Image embedding dimension: 768\n"
          ]
        }
      ],
      "source": [
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "\n",
        "# Load processor (handles image preprocessing and text tokenization)\n",
        "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Load model with optimizations\n",
        "# - torch_dtype=float16: Half precision for 50% memory savings\n",
        "# - attn_implementation=\"sdpa\": PyTorch's efficient attention\n",
        "# - device_map=\"auto\": Automatically place on GPU if available\n",
        "model = AutoModel.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    attn_implementation=\"sdpa\",\n",
        "    device_map=\"auto\"\n",
        ").eval()  # Set to evaluation mode (disables dropout)\n",
        "\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Model device: {model.device}\")\n",
        "print(f\"Image embedding dimension: {model.config.vision_config.hidden_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c80b4a0",
      "metadata": {},
      "source": [
        "## 4. Frame Extraction\n",
        "\n",
        "Extract frames from video at a configurable sample rate using OpenCV.\n",
        "\n",
        "**Key formula**: `timestamp_seconds = frame_index / fps`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dee77051",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_frames(video_path: str, sample_rate: float = SAMPLE_RATE):\n",
        "    \"\"\"\n",
        "    Extract frames from video at specified sample rate.\n",
        "    \n",
        "    Args:\n",
        "        video_path: Path to video file\n",
        "        sample_rate: Frames per second to extract (1.0 = 1 frame/sec)\n",
        "    \n",
        "    Returns:\n",
        "        frames: List of RGB frames (numpy arrays)\n",
        "        metadata: List of dicts with timestamp info\n",
        "        video_info: Dict with video properties\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    \n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"Could not open video: {video_path}\")\n",
        "    \n",
        "    # Get video properties\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    duration = total_frames / fps if fps > 0 else 0\n",
        "    \n",
        "    video_info = {\n",
        "        \"fps\": fps,\n",
        "        \"total_frames\": total_frames,\n",
        "        \"width\": width,\n",
        "        \"height\": height,\n",
        "        \"duration\": duration\n",
        "    }\n",
        "    \n",
        "    print(f\"Video Info:\")\n",
        "    print(f\"  - FPS: {fps:.2f}\")\n",
        "    print(f\"  - Total Frames: {total_frames}\")\n",
        "    print(f\"  - Resolution: {width}x{height}\")\n",
        "    print(f\"  - Duration: {timedelta(seconds=int(duration))}\")\n",
        "    \n",
        "    # Calculate sampling interval\n",
        "    # If video is 30fps and sample_rate is 1.0, we take every 30th frame\n",
        "    frame_interval = max(1, int(fps / sample_rate))\n",
        "    expected_frames = total_frames // frame_interval\n",
        "    print(f\"  - Sampling: 1 frame every {frame_interval} frames ({sample_rate} fps)\")\n",
        "    print(f\"  - Expected extracted frames: ~{expected_frames}\")\n",
        "    \n",
        "    frames = []\n",
        "    metadata = []\n",
        "    frame_idx = 0\n",
        "    \n",
        "    pbar = tqdm(total=total_frames, desc=\"Extracting frames\")\n",
        "    \n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        \n",
        "        if frame_idx % frame_interval == 0:\n",
        "            # Convert BGR (OpenCV default) to RGB (what models expect)\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame_rgb)\n",
        "            \n",
        "            # Calculate timestamp: frame_index / fps = seconds\n",
        "            timestamp = frame_idx / fps\n",
        "            \n",
        "            metadata.append({\n",
        "                \"frame_idx\": frame_idx,\n",
        "                \"timestamp\": timestamp,\n",
        "                \"timestamp_str\": str(timedelta(seconds=int(timestamp))),\n",
        "            })\n",
        "        \n",
        "        frame_idx += 1\n",
        "        pbar.update(1)\n",
        "    \n",
        "    pbar.close()\n",
        "    cap.release()\n",
        "    \n",
        "    print(f\"\\nExtracted {len(frames)} frames\")\n",
        "    return frames, metadata, video_info\n",
        "\n",
        "\n",
        "def get_frame_at_index(video_path: str, frame_idx: int):\n",
        "    \"\"\"Retrieve a single frame from video at specific index (for displaying results).\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "    ret, frame = cap.read()\n",
        "    cap.release()\n",
        "    \n",
        "    if ret:\n",
        "        return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40a8280e",
      "metadata": {},
      "source": [
        "## 5. Embedding Functions\n",
        "\n",
        "Generate embeddings using SigLIP 2:\n",
        "- **Images**: Batch process frames through vision encoder\n",
        "- **Text**: Encode queries through text encoder\n",
        "\n",
        "Both produce vectors in the same 768-dimensional space, enabling cross-modal similarity search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95ac8043",
      "metadata": {},
      "outputs": [],
      "source": [
        "def embed_images(frames: list, batch_size: int = BATCH_SIZE):\n",
        "    \"\"\"\n",
        "    Generate embeddings for a list of frames using SigLIP 2.\n",
        "    \n",
        "    Args:\n",
        "        frames: List of RGB numpy arrays\n",
        "        batch_size: Number of frames to process at once (reduce if OOM)\n",
        "    \n",
        "    Returns:\n",
        "        embeddings: numpy array of shape (n_frames, EMBEDDING_DIM)\n",
        "    \"\"\"\n",
        "    all_embeddings = []\n",
        "    \n",
        "    for i in tqdm(range(0, len(frames), batch_size), desc=\"Generating embeddings\"):\n",
        "        batch = frames[i:i + batch_size]\n",
        "        \n",
        "        # Process batch through the processor (resizes to 256x256, normalizes)\n",
        "        inputs = processor(images=batch, return_tensors=\"pt\").to(model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # Get image features from vision encoder\n",
        "            embeddings = model.get_image_features(**inputs)\n",
        "            \n",
        "            # L2 normalize embeddings (required for cosine similarity)\n",
        "            # This projects all vectors onto the unit sphere\n",
        "            embeddings = embeddings / embeddings.norm(dim=-1, keepdim=True)\n",
        "        \n",
        "        all_embeddings.append(embeddings.cpu())\n",
        "    \n",
        "    return torch.cat(all_embeddings, dim=0).numpy()\n",
        "\n",
        "\n",
        "def embed_text(query: str):\n",
        "    \"\"\"\n",
        "    Generate embedding for a text query using SigLIP 2.\n",
        "    \n",
        "    Args:\n",
        "        query: Natural language search query\n",
        "    \n",
        "    Returns:\n",
        "        embedding: numpy array of shape (1, EMBEDDING_DIM)\n",
        "    \"\"\"\n",
        "    # IMPORTANT: Use padding=\"max_length\" because SigLIP was trained this way\n",
        "    # Without this, results may be suboptimal\n",
        "    inputs = processor(\n",
        "        text=[query], \n",
        "        padding=\"max_length\",  # Critical: model was trained with max_length padding\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Get text features from text encoder\n",
        "        embedding = model.get_text_features(**inputs)\n",
        "        \n",
        "        # L2 normalize (same as images)\n",
        "        embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n",
        "    \n",
        "    return embedding.cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "979cb0a7",
      "metadata": {},
      "source": [
        "## 6. ChromaDB Vector Database\n",
        "\n",
        "Set up persistent vector storage with HNSW index for fast similarity search.\n",
        "\n",
        "**Why ChromaDB?**\n",
        "- Simple Python API\n",
        "- Automatic HNSW indexing (O(log n) search)\n",
        "- Persistent storage to disk\n",
        "- Metadata filtering support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eef46d1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collection: video_frames\n",
            "Current frame count: 0\n"
          ]
        }
      ],
      "source": [
        "# Initialize ChromaDB with persistent storage\n",
        "# Data is automatically saved to disk at DB_DIR\n",
        "chroma_client = chromadb.PersistentClient(path=str(DB_DIR))\n",
        "\n",
        "# Create or get collection for video frames\n",
        "# Using cosine similarity (appropriate for normalized embeddings)\n",
        "collection = chroma_client.get_or_create_collection(\n",
        "    name=\"video_frames\",\n",
        "    metadata={\"hnsw:space\": \"cosine\"}  # Cosine distance for similarity search\n",
        ")\n",
        "\n",
        "print(f\"Collection: {collection.name}\")\n",
        "print(f\"Current frame count: {collection.count()}\")\n",
        "\n",
        "\n",
        "def get_database_stats():\n",
        "    \"\"\"Get statistics about the indexed database.\"\"\"\n",
        "    total_frames = collection.count()\n",
        "    \n",
        "    print(\"\\nDatabase Statistics\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"Total indexed frames: {total_frames}\")\n",
        "    \n",
        "    if total_frames > 0:\n",
        "        # Get unique video IDs\n",
        "        all_data = collection.get(include=[\"metadatas\"])\n",
        "        video_ids = set(m.get(\"video_id\", \"unknown\") for m in all_data[\"metadatas\"])\n",
        "        print(f\"Total videos: {len(video_ids)}\")\n",
        "        print(f\"\\nIndexed videos:\")\n",
        "        for v in video_ids:\n",
        "            print(f\"  - {v}\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "\n",
        "def clear_database():\n",
        "    \"\"\"Clear all data from the collection (use with caution!).\"\"\"\n",
        "    global collection\n",
        "    chroma_client.delete_collection(\"video_frames\")\n",
        "    collection = chroma_client.create_collection(\n",
        "        name=\"video_frames\",\n",
        "        metadata={\"hnsw:space\": \"cosine\"}\n",
        "    )\n",
        "    print(\"Database cleared!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dae229cf",
      "metadata": {},
      "source": [
        "## 7. Video Indexing Pipeline\n",
        "\n",
        "The main indexing function that orchestrates:\n",
        "1. Frame extraction (OpenCV)\n",
        "2. Embedding generation (SigLIP 2)\n",
        "3. Vector storage (ChromaDB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6393066",
      "metadata": {},
      "outputs": [],
      "source": [
        "def index_video(video_path: str, video_id: str, sample_rate: float = SAMPLE_RATE):\n",
        "    \"\"\"\n",
        "    Index a video into ChromaDB for semantic search.\n",
        "    \n",
        "    Args:\n",
        "        video_path: Path to video file\n",
        "        video_id: Unique identifier for this video (used in search results)\n",
        "        sample_rate: Frames per second to extract\n",
        "    \n",
        "    Returns:\n",
        "        num_frames: Number of frames indexed\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Indexing video: {video_id}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    # Step 1: Extract frames from video\n",
        "    frames, metadata, video_info = extract_frames(video_path, sample_rate)\n",
        "    \n",
        "    if len(frames) == 0:\n",
        "        raise ValueError(\"No frames extracted from video\")\n",
        "    \n",
        "    # Step 2: Generate embeddings for all frames\n",
        "    print(\"\\nGenerating embeddings...\")\n",
        "    embeddings = embed_images(frames)\n",
        "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
        "    \n",
        "    # Step 3: Prepare data for ChromaDB\n",
        "    # Create unique IDs for each frame\n",
        "    ids = [f\"{video_id}_frame_{m['frame_idx']}\" for m in metadata]\n",
        "    \n",
        "    # Add video info to metadata\n",
        "    for m in metadata:\n",
        "        m[\"video_id\"] = video_id\n",
        "        m[\"video_path\"] = str(video_path)\n",
        "    \n",
        "    # Step 4: Store in ChromaDB\n",
        "    print(\"\\nStoring in ChromaDB...\")\n",
        "    collection.add(\n",
        "        ids=ids,\n",
        "        embeddings=embeddings.tolist(),\n",
        "        metadatas=metadata\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nSuccessfully indexed {len(frames)} frames from '{video_id}'\")\n",
        "    print(f\"Total frames in database: {collection.count()}\")\n",
        "    \n",
        "    return len(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33c43e1f",
      "metadata": {},
      "source": [
        "## 8. Search Functions\n",
        "\n",
        "Search indexed videos using natural language queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ca284d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def search(query: str, top_k: int = TOP_K):\n",
        "    \"\"\"\n",
        "    Search for frames matching the text query.\n",
        "    \n",
        "    Args:\n",
        "        query: Natural language search query (e.g., \"a person writing on whiteboard\")\n",
        "        top_k: Number of results to return\n",
        "    \n",
        "    Returns:\n",
        "        results: List of dicts with video_id, timestamp, score, etc.\n",
        "    \"\"\"\n",
        "    print(f\"\\nSearching for: '{query}'\")\n",
        "    \n",
        "    # Embed the query text\n",
        "    query_embedding = embed_text(query)\n",
        "    \n",
        "    # Search ChromaDB\n",
        "    results = collection.query(\n",
        "        query_embeddings=query_embedding.tolist(),\n",
        "        n_results=top_k,\n",
        "        include=[\"metadatas\", \"distances\"]\n",
        "    )\n",
        "    \n",
        "    # Format results\n",
        "    search_results = []\n",
        "    for i in range(len(results[\"ids\"][0])):\n",
        "        meta = results[\"metadatas\"][0][i]\n",
        "        distance = results[\"distances\"][0][i]\n",
        "        \n",
        "        # Convert cosine distance to similarity score\n",
        "        # ChromaDB returns distance (0 = identical, 2 = opposite)\n",
        "        # We convert to similarity (1 = identical, -1 = opposite)\n",
        "        score = 1 - distance\n",
        "        \n",
        "        search_results.append({\n",
        "            \"rank\": i + 1,\n",
        "            \"video_id\": meta[\"video_id\"],\n",
        "            \"video_path\": meta.get(\"video_path\", \"\"),\n",
        "            \"timestamp\": meta[\"timestamp\"],\n",
        "            \"timestamp_str\": meta[\"timestamp_str\"],\n",
        "            \"frame_idx\": meta[\"frame_idx\"],\n",
        "            \"score\": score,\n",
        "        })\n",
        "    \n",
        "    print(f\"Found {len(search_results)} results\\n\")\n",
        "    return search_results\n",
        "\n",
        "\n",
        "def display_search_results(results, max_display: int = 5):\n",
        "    \"\"\"\n",
        "    Display search results with frame thumbnails.\n",
        "    \n",
        "    Args:\n",
        "        results: List of result dicts from search()\n",
        "        max_display: Maximum number of results to display\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        print(\"No results to display\")\n",
        "        return\n",
        "    \n",
        "    # Print text results\n",
        "    print(\"Results:\")\n",
        "    print(\"-\" * 70)\n",
        "    for r in results[:max_display]:\n",
        "        print(f\"#{r['rank']} | {r['video_id']} | Time: {r['timestamp_str']} | Score: {r['score']:.4f}\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    # Load and display frames\n",
        "    frames = []\n",
        "    titles = []\n",
        "    \n",
        "    for r in results[:max_display]:\n",
        "        frame = get_frame_at_index(r[\"video_path\"], r[\"frame_idx\"])\n",
        "        if frame is not None:\n",
        "            frames.append(frame)\n",
        "            titles.append(f\"#{r['rank']} {r['timestamp_str']}\\nScore: {r['score']:.3f}\")\n",
        "    \n",
        "    if frames:\n",
        "        # Create figure with subplots\n",
        "        n = len(frames)\n",
        "        cols = min(n, 5)\n",
        "        rows = (n + cols - 1) // cols\n",
        "        \n",
        "        fig, axes = plt.subplots(rows, cols, figsize=(15, 3 * rows))\n",
        "        if n == 1:\n",
        "            axes = [axes]\n",
        "        else:\n",
        "            axes = axes.flatten() if rows > 1 else axes\n",
        "        \n",
        "        for i, ax in enumerate(axes):\n",
        "            if i < n:\n",
        "                ax.imshow(frames[i])\n",
        "                ax.set_title(titles[i], fontsize=9)\n",
        "            ax.axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8452e96",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Demo Usage\n",
        "\n",
        "Now let's use the pipeline to index and search videos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80cb5cfb",
      "metadata": {},
      "source": [
        "## Index a Video\n",
        "\n",
        "Update the `VIDEO_PATH` below to point to your video file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e55e91f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video not found: ./data/videos/video.mp4\n",
            "\n",
            "Please update VIDEO_PATH to point to your video file.\n",
            "You can place videos in: /content/data/videos\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# UPDATE THESE VALUES FOR YOUR VIDEO\n",
        "# =============================================================================\n",
        "\n",
        "VIDEO_PATH = \"./data/videos/video.mp4\"  # Path to your video file\n",
        "VIDEO_ID = \"sample_video\"                 # Unique identifier for this video\n",
        "\n",
        "# =============================================================================\n",
        "\n",
        "# Check if video exists and index it\n",
        "if Path(VIDEO_PATH).exists():\n",
        "    num_frames = index_video(VIDEO_PATH, VIDEO_ID, sample_rate=SAMPLE_RATE)\n",
        "else:\n",
        "    print(f\"Video not found: {VIDEO_PATH}\")\n",
        "    print(\"\\nPlease update VIDEO_PATH to point to your video file.\")\n",
        "    print(f\"You can place videos in: {VIDEOS_DIR.absolute()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5667b209",
      "metadata": {},
      "source": [
        "## Check Database Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "379d54e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "get_database_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b46f47bb",
      "metadata": {},
      "source": [
        "## Search Videos\n",
        "\n",
        "Try different natural language queries to find matching moments in your indexed videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df526f70",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example search query - update this to match your video content\n",
        "query = \"a person talking\"\n",
        "\n",
        "results = search(query, top_k=5)\n",
        "display_search_results(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9073b02d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try another query\n",
        "query = \"outdoor scene with nature\"\n",
        "\n",
        "results = search(query, top_k=5)\n",
        "display_search_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf88e105",
      "metadata": {},
      "source": [
        "## Batch Search (Multiple Queries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "898ef437",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test multiple queries at once\n",
        "test_queries = [\n",
        "    \"a person smiling\",\n",
        "    \"text on screen\",\n",
        "    \"close up of face\",\n",
        "    \"wide shot\",\n",
        "    \"bright colors\",\n",
        "]\n",
        "\n",
        "for q in test_queries:\n",
        "    results = search(q, top_k=3)\n",
        "    if results:\n",
        "        print(f\"Query: '{q}'\")\n",
        "        for r in results:\n",
        "            print(f\"  -> {r['timestamp_str']} (score: {r['score']:.3f})\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0495b4ce",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Clean Up (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "246d387e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to clear all indexed data\n",
        "# clear_database()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "349d529b",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook implements a complete video semantic search pipeline:\n",
        "\n",
        "| Step | Function | Description |\n",
        "|------|----------|-------------|\n",
        "| 1 | `extract_frames()` | Extract frames from video at configurable FPS using OpenCV |\n",
        "| 2 | `embed_images()` | Generate 768-dim SigLIP 2 embeddings for frames |\n",
        "| 3 | `embed_text()` | Generate 768-dim SigLIP 2 embedding for text query |\n",
        "| 4 | `index_video()` | Full indexing pipeline (extract -> embed -> store) |\n",
        "| 5 | `search()` | Find matching frames using cosine similarity |\n",
        "| 6 | `display_search_results()` | Visualize results with thumbnails |\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "| Parameter | Value | Purpose |\n",
        "|-----------|-------|---------|\n",
        "| `MODEL_NAME` | `google/siglip2-base-patch16-256` | Vision-language encoder |\n",
        "| `SAMPLE_RATE` | `1.0` | Frames per second to extract |\n",
        "| `BATCH_SIZE` | `16` | Frames per embedding batch |\n",
        "| `TOP_K` | `10` | Search results to return |\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Index more videos to build a larger searchable collection\n",
        "2. Experiment with different sample rates for your use case\n",
        "3. Build a Streamlit app with video upload and playback"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
